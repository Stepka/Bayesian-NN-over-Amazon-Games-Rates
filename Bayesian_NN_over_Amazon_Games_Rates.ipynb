{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian NN over Amazon Games Rates.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stepka/Bayesian-NN-over-Amazon-Games-Rates/blob/master/Bayesian_NN_over_Amazon_Games_Rates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SHa2ruWLIUmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "# !pip3 uninstall tensorflow\n",
        "# !pip3 uninstall tensorflow-probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6meW3ygcNbu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import tensorflow_probability as tfp\n",
        "\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n",
        "# print(\"TensorFlow probability version:\", tfp.__version__)\n",
        "# print(\"Eager mode:\", tf.executing_eagerly())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0vYxN8v3c6y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet tensorflow==1.13.1\n",
        "!pip3 install --quiet --upgrade tensorflow-probability\n",
        "!pip3 install --quiet tensorflow-hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCRxnhtUAzu2",
        "colab_type": "code",
        "outputId": "a911189d-6ed6-41df-bdbb-17b6c2b7d5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0409 16:18:34.293462 140269118805888 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hqrNiXPY32Qv",
        "colab_type": "code",
        "outputId": "492a548b-51b5-4fe5-bec6-21bb63cb1aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow probability version:\", tfp.__version__)\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "tf.executing_eagerly()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.13.1\n",
            "TensorFlow probability version: 0.6.0\n",
            "Pandas version: 0.22.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8k53ZSbjpWIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5bd352fd-d11a-4d4b-a61d-4b742ef87418"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "default_path = \"/drive/My Drive/Colab Notebooks/Bayesian Amazon Video Rates/data/\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0XAfSJMJ7XlI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################\n",
        "###   load data    ###\n",
        "######################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zOefKnu6D-zQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_json = pd.read_json(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\", lines=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3-1lE-h7D-zX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "cf56f219-ab55-442a-eff2-60fd70f34fa6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Num rows: {}\\n\".format(len(dataset_json.index)))\n",
        "\n",
        "dataset_json.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num rows: 231780\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asin</th>\n",
              "      <th>helpful</th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[8, 12]</td>\n",
              "      <td>1</td>\n",
              "      <td>Installing the game was a struggle (because of...</td>\n",
              "      <td>07 9, 2012</td>\n",
              "      <td>A2HD75EMZR8QLN</td>\n",
              "      <td>123</td>\n",
              "      <td>Pay to unlock content? I don't think so.</td>\n",
              "      <td>1341792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>4</td>\n",
              "      <td>If you like rally cars get this game you will ...</td>\n",
              "      <td>06 30, 2013</td>\n",
              "      <td>A3UR8NLLY1ZHCX</td>\n",
              "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
              "      <td>Good rally game</td>\n",
              "      <td>1372550400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>1st shipment received a book instead of the ga...</td>\n",
              "      <td>06 28, 2014</td>\n",
              "      <td>A1INA0F5CWW3J4</td>\n",
              "      <td>Amazon Shopper \"Mr.Repsol\"</td>\n",
              "      <td>Wrong key</td>\n",
              "      <td>1403913600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[7, 10]</td>\n",
              "      <td>3</td>\n",
              "      <td>I got this version instead of the PS3 version,...</td>\n",
              "      <td>09 14, 2011</td>\n",
              "      <td>A1DLMTOTHQ4AST</td>\n",
              "      <td>ampgreen</td>\n",
              "      <td>awesome game, if it did not crash frequently !!</td>\n",
              "      <td>1315958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[2, 2]</td>\n",
              "      <td>4</td>\n",
              "      <td>I had Dirt 2 on Xbox 360 and it was an okay ga...</td>\n",
              "      <td>06 14, 2011</td>\n",
              "      <td>A361M14PU2GUEG</td>\n",
              "      <td>Angry Ryan \"Ryan A. Forrest\"</td>\n",
              "      <td>DIRT 3</td>\n",
              "      <td>1308009600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>4</td>\n",
              "      <td>Overall this is a well done racing game, with ...</td>\n",
              "      <td>05 11, 2013</td>\n",
              "      <td>A2UTRVO4FDCBH6</td>\n",
              "      <td>A.R.G.</td>\n",
              "      <td>Good racing game, terrible Windows Live Requir...</td>\n",
              "      <td>1368230400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[11, 13]</td>\n",
              "      <td>5</td>\n",
              "      <td>Loved playing Dirt 2 and I thought the graphic...</td>\n",
              "      <td>08 14, 2011</td>\n",
              "      <td>AN3YYDZAS3O1Y</td>\n",
              "      <td>Bob</td>\n",
              "      <td>A step up from Dirt 2 and that is terrific!</td>\n",
              "      <td>1313280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[1, 4]</td>\n",
              "      <td>1</td>\n",
              "      <td>I can't tell you what a piece of dog**** this ...</td>\n",
              "      <td>11 24, 2012</td>\n",
              "      <td>AQTC623NCESZW</td>\n",
              "      <td>Chesty Puller</td>\n",
              "      <td>Crash 3 is correct name AKA Microsoft</td>\n",
              "      <td>1353715200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>4</td>\n",
              "      <td>I initially gave this one star because it was ...</td>\n",
              "      <td>11 14, 2012</td>\n",
              "      <td>A1QJJU33VNC4S7</td>\n",
              "      <td>D@rkFX</td>\n",
              "      <td>A great game ruined by Microsoft's account man...</td>\n",
              "      <td>1352851200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0700099867</td>\n",
              "      <td>[1, 1]</td>\n",
              "      <td>2</td>\n",
              "      <td>I still haven't figured this one out. Did ever...</td>\n",
              "      <td>02 8, 2014</td>\n",
              "      <td>A2JLT2WY0F2HVI</td>\n",
              "      <td>D. Sweetapple</td>\n",
              "      <td>Couldn't get this one to work</td>\n",
              "      <td>1391817600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         asin   helpful  overall  \\\n",
              "0  0700099867   [8, 12]        1   \n",
              "1  0700099867    [0, 0]        4   \n",
              "2  0700099867    [0, 0]        1   \n",
              "3  0700099867   [7, 10]        3   \n",
              "4  0700099867    [2, 2]        4   \n",
              "5  0700099867    [0, 0]        4   \n",
              "6  0700099867  [11, 13]        5   \n",
              "7  0700099867    [1, 4]        1   \n",
              "8  0700099867    [0, 1]        4   \n",
              "9  0700099867    [1, 1]        2   \n",
              "\n",
              "                                          reviewText   reviewTime  \\\n",
              "0  Installing the game was a struggle (because of...   07 9, 2012   \n",
              "1  If you like rally cars get this game you will ...  06 30, 2013   \n",
              "2  1st shipment received a book instead of the ga...  06 28, 2014   \n",
              "3  I got this version instead of the PS3 version,...  09 14, 2011   \n",
              "4  I had Dirt 2 on Xbox 360 and it was an okay ga...  06 14, 2011   \n",
              "5  Overall this is a well done racing game, with ...  05 11, 2013   \n",
              "6  Loved playing Dirt 2 and I thought the graphic...  08 14, 2011   \n",
              "7  I can't tell you what a piece of dog**** this ...  11 24, 2012   \n",
              "8  I initially gave this one star because it was ...  11 14, 2012   \n",
              "9  I still haven't figured this one out. Did ever...   02 8, 2014   \n",
              "\n",
              "       reviewerID                        reviewerName  \\\n",
              "0  A2HD75EMZR8QLN                                 123   \n",
              "1  A3UR8NLLY1ZHCX  Alejandro Henao \"Electronic Junky\"   \n",
              "2  A1INA0F5CWW3J4          Amazon Shopper \"Mr.Repsol\"   \n",
              "3  A1DLMTOTHQ4AST                            ampgreen   \n",
              "4  A361M14PU2GUEG        Angry Ryan \"Ryan A. Forrest\"   \n",
              "5  A2UTRVO4FDCBH6                              A.R.G.   \n",
              "6   AN3YYDZAS3O1Y                                 Bob   \n",
              "7   AQTC623NCESZW                       Chesty Puller   \n",
              "8  A1QJJU33VNC4S7                              D@rkFX   \n",
              "9  A2JLT2WY0F2HVI                       D. Sweetapple   \n",
              "\n",
              "                                             summary  unixReviewTime  \n",
              "0           Pay to unlock content? I don't think so.      1341792000  \n",
              "1                                    Good rally game      1372550400  \n",
              "2                                          Wrong key      1403913600  \n",
              "3    awesome game, if it did not crash frequently !!      1315958400  \n",
              "4                                             DIRT 3      1308009600  \n",
              "5  Good racing game, terrible Windows Live Requir...      1368230400  \n",
              "6        A step up from Dirt 2 and that is terrific!      1313280000  \n",
              "7              Crash 3 is correct name AKA Microsoft      1353715200  \n",
              "8  A great game ruined by Microsoft's account man...      1352851200  \n",
              "9                      Couldn't get this one to work      1391817600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "WoCwJYwsklpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_json.drop(labels = [\"asin\", \"helpful\", \"reviewText\",  \"reviewTime\", \"reviewerID\", \"reviewerName\", \"unixReviewTime\"], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ilupL8RshaI-",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\", \"https://tfhub.dev/google/nnlm-en-dim128/1\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XdLEngCNAzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "8fa7892b-a0db-4f99-f591-35820552b258"
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0409 16:20:22.724450 140269118805888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QxVWrc_Qb1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "def extract_embedings(messages, name, use_cache=True):\n",
        "\n",
        "  # Reduce logging output.\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  print(\"Messages  length: {}\\n\".format(len(messages)))\n",
        "\n",
        "  EMBEDINGS_BATCH_SIZE = 10000\n",
        "\n",
        "  message_embeddings = []\n",
        "  start_step = 0\n",
        "\n",
        "  if use_cache:\n",
        "    try:\n",
        "      message_embeddings_df = pd.read_csv(default_path + name + '_embedings.csv')\n",
        "    except FileNotFoundError:\n",
        "      print(\"no saved embedings\")\n",
        "      message_embeddings_df = None\n",
        "\n",
        "    if message_embeddings_df is not None: \n",
        "      print(\"we have\", len(message_embeddings_df.index), \"saved embedings\")\n",
        "      start_step = int(len(message_embeddings_df.index) / EMBEDINGS_BATCH_SIZE)\n",
        "      print(\"will start from\", start_step, \"step\")\n",
        "      message_embeddings = message_embeddings_df.values\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "\n",
        "    num_steps = int( 1 + len(messages) / EMBEDINGS_BATCH_SIZE )\n",
        "    print(\"Num steps: {}\\n\".format(num_steps))\n",
        "    if start_step < num_steps and len(message_embeddings) < len(messages):\n",
        "      for step in range(start_step, num_steps):\n",
        "        embeddings = session.run(embed(messages[EMBEDINGS_BATCH_SIZE*step : EMBEDINGS_BATCH_SIZE*(step+1)]))\n",
        "    #     message_embeddings = message_embeddings + embeddings\n",
        "        if len(message_embeddings) == 0:\n",
        "          message_embeddings = embeddings\n",
        "        else :\n",
        "          message_embeddings = np.vstack((message_embeddings, embeddings))\n",
        "\n",
        "        message_embeddings_df = pd.DataFrame(data = message_embeddings)\n",
        "  #       pd.DataFrame(message_embeddings_df).to_csv(default_path + name + '_embedings.csv', index=False, header=True)\n",
        "\n",
        "  #       if step % 10 == 0:\n",
        "        pd.DataFrame(message_embeddings_df).to_csv(default_path + name + '_embedings ' + str(step) + '.csv', index=False, header=True)\n",
        "\n",
        "        print(\"Step {}, Embedding size: {}\".format(step, len(message_embeddings)))\n",
        "      \n",
        "        if len(message_embeddings) == len(messages):\n",
        "          pd.DataFrame(message_embeddings_df).to_csv(default_path + name + '_embedings.csv', index=False, header=True)\n",
        "          \n",
        "    else:\n",
        "      print(\"We have all data\")\n",
        "      \n",
        "          \n",
        "      \n",
        "  return message_embeddings, message_embeddings_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkxH93y890OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fd0cad75-cb06-4994-afea-2fe4172bd470"
      },
      "cell_type": "code",
      "source": [
        "embedings, embedings_df = extract_embedings(dataset_json[\"summary\"].values, \"summary\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Messages  length: 231780\n",
            "\n",
            "we have 231780 saved embedings\n",
            "will start from 23 step\n",
            "Num steps: 24\n",
            "\n",
            "We have all data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_SYdaGK35dab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   df = pd.read_csv(default_path + 'summary_embedings.csv')\n",
        "# except FileNotFoundError:\n",
        "#   print(\"no file\")\n",
        "#   df = None\n",
        "\n",
        "# if df is not None: \n",
        "#   print(len(df.index))\n",
        "  \n",
        "# df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBUbTFV3IP4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "497607c2-03f0-44f4-83e9-022f52da1130"
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.concat([dataset_json, embedings_df], axis=1)\n",
        "\n",
        "dataset.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>summary</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Pay to unlock content? I don't think so.</td>\n",
              "      <td>0.046938</td>\n",
              "      <td>0.042071</td>\n",
              "      <td>0.024881</td>\n",
              "      <td>0.045308</td>\n",
              "      <td>-0.019535</td>\n",
              "      <td>0.081542</td>\n",
              "      <td>0.083805</td>\n",
              "      <td>0.071415</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010543</td>\n",
              "      <td>0.037760</td>\n",
              "      <td>-0.004364</td>\n",
              "      <td>-0.044930</td>\n",
              "      <td>-0.047975</td>\n",
              "      <td>-0.018246</td>\n",
              "      <td>-0.048304</td>\n",
              "      <td>-0.070092</td>\n",
              "      <td>-0.066281</td>\n",
              "      <td>0.050676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Good rally game</td>\n",
              "      <td>0.016563</td>\n",
              "      <td>-0.031776</td>\n",
              "      <td>-0.043782</td>\n",
              "      <td>-0.042133</td>\n",
              "      <td>0.021630</td>\n",
              "      <td>-0.033483</td>\n",
              "      <td>0.011921</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031585</td>\n",
              "      <td>0.049901</td>\n",
              "      <td>-0.031180</td>\n",
              "      <td>-0.048357</td>\n",
              "      <td>-0.077174</td>\n",
              "      <td>0.016070</td>\n",
              "      <td>0.042710</td>\n",
              "      <td>-0.037952</td>\n",
              "      <td>-0.028943</td>\n",
              "      <td>0.030393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Wrong key</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>-0.011373</td>\n",
              "      <td>-0.010637</td>\n",
              "      <td>0.014312</td>\n",
              "      <td>-0.035867</td>\n",
              "      <td>-0.022187</td>\n",
              "      <td>-0.002389</td>\n",
              "      <td>-0.061504</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004081</td>\n",
              "      <td>-0.001812</td>\n",
              "      <td>-0.025526</td>\n",
              "      <td>-0.063241</td>\n",
              "      <td>-0.036931</td>\n",
              "      <td>0.018444</td>\n",
              "      <td>0.039002</td>\n",
              "      <td>0.050907</td>\n",
              "      <td>-0.056628</td>\n",
              "      <td>-0.006743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>awesome game, if it did not crash frequently !!</td>\n",
              "      <td>0.041646</td>\n",
              "      <td>-0.020160</td>\n",
              "      <td>0.010884</td>\n",
              "      <td>0.035234</td>\n",
              "      <td>-0.005660</td>\n",
              "      <td>-0.006740</td>\n",
              "      <td>0.054527</td>\n",
              "      <td>0.017031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030309</td>\n",
              "      <td>-0.013751</td>\n",
              "      <td>-0.054002</td>\n",
              "      <td>0.061169</td>\n",
              "      <td>-0.031388</td>\n",
              "      <td>-0.077839</td>\n",
              "      <td>-0.043388</td>\n",
              "      <td>-0.047516</td>\n",
              "      <td>0.072083</td>\n",
              "      <td>0.038782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>DIRT 3</td>\n",
              "      <td>0.023018</td>\n",
              "      <td>-0.029862</td>\n",
              "      <td>-0.074362</td>\n",
              "      <td>-0.023005</td>\n",
              "      <td>0.023212</td>\n",
              "      <td>0.066392</td>\n",
              "      <td>0.032889</td>\n",
              "      <td>0.023983</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044928</td>\n",
              "      <td>-0.003295</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>-0.032011</td>\n",
              "      <td>-0.064896</td>\n",
              "      <td>-0.062261</td>\n",
              "      <td>0.027037</td>\n",
              "      <td>-0.014742</td>\n",
              "      <td>0.011601</td>\n",
              "      <td>0.090250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 514 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall                                          summary         0  \\\n",
              "0        1         Pay to unlock content? I don't think so.  0.046938   \n",
              "1        4                                  Good rally game  0.016563   \n",
              "2        1                                        Wrong key  0.019536   \n",
              "3        3  awesome game, if it did not crash frequently !!  0.041646   \n",
              "4        4                                           DIRT 3  0.023018   \n",
              "\n",
              "          1         2         3         4         5         6         7  \\\n",
              "0  0.042071  0.024881  0.045308 -0.019535  0.081542  0.083805  0.071415   \n",
              "1 -0.031776 -0.043782 -0.042133  0.021630 -0.033483  0.011921  0.006047   \n",
              "2 -0.011373 -0.010637  0.014312 -0.035867 -0.022187 -0.002389 -0.061504   \n",
              "3 -0.020160  0.010884  0.035234 -0.005660 -0.006740  0.054527  0.017031   \n",
              "4 -0.029862 -0.074362 -0.023005  0.023212  0.066392  0.032889  0.023983   \n",
              "\n",
              "     ...          502       503       504       505       506       507  \\\n",
              "0    ...     0.010543  0.037760 -0.004364 -0.044930 -0.047975 -0.018246   \n",
              "1    ...    -0.031585  0.049901 -0.031180 -0.048357 -0.077174  0.016070   \n",
              "2    ...    -0.004081 -0.001812 -0.025526 -0.063241 -0.036931  0.018444   \n",
              "3    ...    -0.030309 -0.013751 -0.054002  0.061169 -0.031388 -0.077839   \n",
              "4    ...    -0.044928 -0.003295  0.002224 -0.032011 -0.064896 -0.062261   \n",
              "\n",
              "        508       509       510       511  \n",
              "0 -0.048304 -0.070092 -0.066281  0.050676  \n",
              "1  0.042710 -0.037952 -0.028943  0.030393  \n",
              "2  0.039002  0.050907 -0.056628 -0.006743  \n",
              "3 -0.043388 -0.047516  0.072083  0.038782  \n",
              "4  0.027037 -0.014742  0.011601  0.090250  \n",
              "\n",
              "[5 rows x 514 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "0sQ7d0dkav_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2f93f280-4f39-42f3-fdda-ceb63296e34a"
      },
      "cell_type": "code",
      "source": [
        "summaries = dataset[\"summary\"]\n",
        "\n",
        "print(dataset['overall'][9000], summaries[9000])\n",
        "print(dataset['overall'][9001], summaries[9001])\n",
        "print(dataset['overall'][9002], summaries[9002])\n",
        "print(dataset['overall'][9003], summaries[9003])\n",
        "print(dataset['overall'][9004], summaries[9004])\n",
        "print(dataset['overall'][9005], summaries[9005])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 A game to rent, but not to buy\n",
            "3 Fun if you love pokemon.\n",
            "2 A big thumbs down\n",
            "4 a pretty cool game\n",
            "5 The #1 arcade smash is back!  Upright funny!\n",
            "4 Fun for all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TmG1U4OgJ8dY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.drop(labels = [\"summary\"], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0H327geiQnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "31adf401-e36b-478c-8eda-406c62d27725"
      },
      "cell_type": "code",
      "source": [
        "embeding_columns = list(dataset)\n",
        "score_column = 'overall'\n",
        "embeding_columns.remove(score_column)\n",
        "\n",
        "print(\"features: \", embeding_columns)\n",
        "print(\"target feature: \", score_column)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features:  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511']\n",
            "target feature:  overall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUWTnU_QLTkZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Probability Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\"\"\"Trains a Bayesian neural network to classify MNIST digits.\n",
        "The architecture is LeNet-5 [1].\n",
        "#### References\n",
        "[1]: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
        "     Gradient-based learning applied to document recognition.\n",
        "     _Proceedings of the IEEE_, 1998.\n",
        "     http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Dependency imports\n",
        "from absl import flags\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
        "from matplotlib.backends import backend_agg\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "IMAGE_SHAPE = [28, 28, 1]\n",
        "\n",
        "####Delete all flags before declare#####\n",
        "\n",
        "# def del_all_flags(FLAGS):\n",
        "#     flags_dict = FLAGS._flags()    \n",
        "#     keys_list = [keys for keys in flags_dict]    \n",
        "#     for keys in keys_list:\n",
        "#         FLAGS.__delattr__(keys)\n",
        "\n",
        "# del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "LEARNING_RATE=0.001\n",
        "MAX_STEPS=6000\n",
        "BATCH_SIZE=128\n",
        "DATA_DIR=os.path.join(os.getenv(\"CONTENT\", \"/content\"), \"bayesian_amazon/data\")\n",
        "MODEL_DIR=os.path.join(os.getenv(\"CONTENT\", \"/content\"), \"bayesian_amazon/\")\n",
        "VIZ_STEPS=400\n",
        "NUM_MONTE_CARLO=50\n",
        "VALIDATION_SIZE = 10000\n",
        "\n",
        "\n",
        "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
        "  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
        "  Args:\n",
        "    names: A Python `iterable` of `str` variable names.\n",
        "    qm_vals: A Python `iterable`, the same length as `names`,\n",
        "      whose elements are Numpy `array`s, of any shape, containing\n",
        "      posterior means of weight varibles.\n",
        "    qs_vals: A Python `iterable`, the same length as `names`,\n",
        "      whose elements are Numpy `array`s, of any shape, containing\n",
        "      posterior standard deviations of weight varibles.\n",
        "    fname: Python `str` filename to save the plot to.\n",
        "  \"\"\"\n",
        "  fig = figure.Figure(figsize=(6, 3))\n",
        "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  for n, qm in zip(names, qm_vals):\n",
        "    sns.distplot(qm.flatten(), ax=ax, label=n)\n",
        "  ax.set_title(\"weight means\")\n",
        "  ax.set_xlim([-1.5, 1.5])\n",
        "  ax.legend()\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 2)\n",
        "  for n, qs in zip(names, qs_vals):\n",
        "    sns.distplot(qs.flatten(), ax=ax)\n",
        "  ax.set_title(\"weight stddevs\")\n",
        "  ax.set_xlim([0, 1.])\n",
        "\n",
        "  fig.tight_layout()\n",
        "  canvas.print_figure(fname, format=\"png\")\n",
        "  print(\"saved {}\".format(fname))\n",
        "\n",
        "\n",
        "def plot_heldout_prediction(start_row, probs,\n",
        "                            fname, n=10, title=\"\"):\n",
        "  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n",
        "  Args:\n",
        "    input_vals: \n",
        "    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n",
        "      num_heldout, num_classes]` containing Monte Carlo samples of\n",
        "      class probabilities for each heldout sample.\n",
        "    fname: Python `str` filename to save the plot to.\n",
        "    n: Python `int` number of datapoints to vizualize.\n",
        "    title: Python `str` title for the plot.\n",
        "  \"\"\"\n",
        "  fig = figure.Figure(figsize=(9, 3*n))\n",
        "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
        "  \n",
        "\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 1)\n",
        "#     ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n",
        "    # axes coordinates are 0,0 is bottom left and 1,1 is upper right\n",
        "    ax.text(0.5, 0.5, \n",
        "            summaries[start_row + i],\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='center', \n",
        "            wrap=True,\n",
        "            fontsize=10,\n",
        "            transform=ax.transAxes)\n",
        "\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 2)\n",
        "    for prob_sample in probs:\n",
        "      sns.barplot(np.arange(6), prob_sample[i, :], alpha=0.1, ax=ax)\n",
        "      ax.set_ylim([0, 1])\n",
        "    ax.set_title(\"posterior samples\")\n",
        "\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 3)\n",
        "    sns.barplot(np.arange(6), np.mean(probs[:, i, :], axis=0), ax=ax)\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_title(\"predictive probs\")\n",
        "  fig.suptitle(title)\n",
        "  fig.tight_layout()\n",
        "\n",
        "  canvas.print_figure(fname, format=\"png\")\n",
        "  print(\"saved {}\".format(fname))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgr1LXlQVHqO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_input_pipeline(df, features, target_feature, batch_size, heldout_size):\n",
        "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
        "\n",
        "  train_X = df[features][heldout_size:]\n",
        "  train_y = df[target_feature][heldout_size:]\n",
        "  \n",
        "  validation_X = df[features][:heldout_size]\n",
        "  validation_y = df[target_feature][:heldout_size]\n",
        "  \n",
        "  # Build an iterator over training batches.\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      (\n",
        "      tf.cast(train_X.values, tf.float32), \n",
        "      tf.cast(train_y.values, tf.int32)\n",
        "      )\n",
        "  )\n",
        "  training_batches = training_dataset.shuffle(\n",
        "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
        "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
        "\n",
        "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      (\n",
        "      tf.cast(validation_X.values, tf.float32), \n",
        "      tf.cast(validation_y.values, tf.int32)\n",
        "      )\n",
        "  )\n",
        "  heldout_frozen = (heldout_dataset.take(heldout_size).\n",
        "                    repeat().batch(heldout_size))\n",
        "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_frozen)\n",
        "\n",
        "  # Combine these into a feedable iterator that can switch between training\n",
        "  # and validation inputs.\n",
        "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
        "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
        "      handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  feature_sets, labels = feedable_iterator.get_next()\n",
        "\n",
        "  return feature_sets, labels, handle, training_iterator, heldout_iterator\n",
        "\n",
        "\n",
        "def main(train_df, features, target_feature):\n",
        "#   del argv  # unused\n",
        "  if tf.io.gfile.exists(MODEL_DIR):\n",
        "    tf.compat.v1.logging.warning(\n",
        "        \"Warning: deleting old log directory at {}\".format(MODEL_DIR))\n",
        "    tf.io.gfile.rmtree(MODEL_DIR)\n",
        "  tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "  ############################################################################\n",
        "  \n",
        "#   mnist_data = mnist.read_data_sets(FLAGS.data_dir, reshape=False)\n",
        "\n",
        "  (embedings, labels, handle,\n",
        "   training_iterator, heldout_iterator) = build_input_pipeline(\n",
        "       train_df, features, target_feature, \n",
        "      BATCH_SIZE, VALIDATION_SIZE)\n",
        "\n",
        "  ############################################################################\n",
        "  # Build a Bayesian LeNet5 network. We use the Flipout Monte Carlo estimator\n",
        "  # for the convolution and fully-connected layers: this enables lower\n",
        "  # variance stochastic gradients than naive reparameterization.\n",
        "  with tf.compat.v1.name_scope(\"bayesian_neural_net\", values=[embedings]):\n",
        "    neural_net = tf.keras.Sequential([\n",
        "        tfp.layers.DenseFlipout(512, activation=tf.nn.relu),\n",
        "        tfp.layers.DenseFlipout(6)\n",
        "        ])\n",
        "\n",
        "    logits = neural_net(embedings)\n",
        "    labels_distribution = tfd.Categorical(logits=logits)\n",
        "    log_probs = labels_distribution.log_prob(labels)\n",
        "\n",
        "\n",
        "  \n",
        "  #############################################################################\n",
        "  \n",
        "\n",
        "  # Compute the -ELBO as the loss, averaged over the batch size.\n",
        "  neg_log_likelihood = -tf.reduce_mean(\n",
        "      input_tensor=labels_distribution.log_prob(labels))\n",
        "  kl = sum(neural_net.losses) / (len(train_df.index) - VALIDATION_SIZE)\n",
        "  elbo_loss = neg_log_likelihood + kl\n",
        "\n",
        "  # Build metrics for evaluation. Predictions are formed from a single forward\n",
        "  # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
        "  predictions = tf.argmax(input=logits, axis=1)\n",
        "  accuracy, accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
        "      labels=labels, predictions=predictions)\n",
        "\n",
        "  # Extract weight posterior statistics for layers with weight distributions\n",
        "  # for later visualization.\n",
        "  names = []\n",
        "  qmeans = []\n",
        "  qstds = []\n",
        "  for i, layer in enumerate(neural_net.layers):\n",
        "    try:\n",
        "      q = layer.kernel_posterior\n",
        "    except AttributeError:\n",
        "      continue\n",
        "    names.append(\"Layer {}\".format(i))\n",
        "    qmeans.append(q.mean())\n",
        "    qstds.append(q.stddev())\n",
        "\n",
        "  with tf.compat.v1.name_scope(\"train\"):\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(\n",
        "        learning_rate=LEARNING_RATE)\n",
        "    train_op = optimizer.minimize(elbo_loss)\n",
        "\n",
        "  init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
        "                     tf.compat.v1.local_variables_initializer())\n",
        "  \n",
        "###########################################################################\n",
        "  \n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "\n",
        "    # Run the training loop.\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "    for step in range(MAX_STEPS):\n",
        "      _ = sess.run([train_op, accuracy_update_op],\n",
        "                   feed_dict={handle: train_handle})\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        loss_value, accuracy_value = sess.run(\n",
        "            [elbo_loss, accuracy], feed_dict={handle: train_handle})\n",
        "        print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\n",
        "            step, loss_value, accuracy_value))\n",
        "\n",
        "      if (step+1) % VIZ_STEPS == 0:\n",
        "        # Compute log prob of heldout set by averaging draws from the model:\n",
        "        # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
        "        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
        "        # where model_i is a draw from the posterior p(model|train).\n",
        "        probs = np.asarray([sess.run((labels_distribution.probs),\n",
        "                                     feed_dict={handle: heldout_handle})\n",
        "                            for _ in range(NUM_MONTE_CARLO)])\n",
        "        mean_probs = np.mean(probs, axis=0)\n",
        "\n",
        "        embedings_vals, label_vals = sess.run((embedings, labels),\n",
        "                                          feed_dict={handle: heldout_handle})\n",
        "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
        "                                               label_vals.flatten()]))\n",
        "        print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
        "\n",
        "        qm_vals, qs_vals = sess.run((qmeans, qstds))\n",
        "\n",
        "        plot_weight_posteriors(names, qm_vals, qs_vals,\n",
        "                               fname=os.path.join(\n",
        "                                   MODEL_DIR,\n",
        "                                   \"step{:05d}_weights.png\".format(step)))\n",
        "\n",
        "        plot_heldout_prediction(len(train_df.index) - VALIDATION_SIZE, probs,\n",
        "                                fname=os.path.join(\n",
        "                                    MODEL_DIR,\n",
        "                                    \"step{:05d}_pred.png\".format(step)),\n",
        "                                title=\"mean heldout logprob {:.2f}\"\n",
        "                                .format(heldout_lp))\n",
        "# if __name__ == \"__main__\":\n",
        "#   tf.compat.v1.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyEDa9ZfOGrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1887
        },
        "outputId": "c8bf31a9-6b77-44a6-d60f-0cf97ee0ebc5"
      },
      "cell_type": "code",
      "source": [
        "main(dataset[:150000], embeding_columns, score_column)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:   0 Loss: 6.477 Accuracy: 0.109\n",
            "Step: 100 Loss: 5.683 Accuracy: 0.527\n",
            "Step: 200 Loss: 5.490 Accuracy: 0.546\n",
            "Step: 300 Loss: 5.355 Accuracy: 0.553\n",
            " ... Held-out nats: -0.903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
            "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
            "  alternative=\"'density'\", removal=\"3.1\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saved /content/bayesian_amazon/step00399_weights.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1428: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
            "  stat_data = remove_na(group_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saved /content/bayesian_amazon/step00399_pred.png\n",
            "Step: 400 Loss: 5.390 Accuracy: 0.557\n",
            "Step: 500 Loss: 5.277 Accuracy: 0.562\n",
            "Step: 600 Loss: 5.094 Accuracy: 0.564\n",
            "Step: 700 Loss: 4.909 Accuracy: 0.568\n",
            " ... Held-out nats: -0.905\n",
            "saved /content/bayesian_amazon/step00799_weights.png\n",
            "saved /content/bayesian_amazon/step00799_pred.png\n",
            "Step: 800 Loss: 5.038 Accuracy: 0.570\n",
            "Step: 900 Loss: 4.926 Accuracy: 0.571\n",
            "Step: 1000 Loss: 4.637 Accuracy: 0.573\n",
            "Step: 1100 Loss: 4.777 Accuracy: 0.574\n",
            " ... Held-out nats: -0.889\n",
            "saved /content/bayesian_amazon/step01199_weights.png\n",
            "saved /content/bayesian_amazon/step01199_pred.png\n",
            "Step: 1200 Loss: 4.520 Accuracy: 0.575\n",
            "Step: 1300 Loss: 4.549 Accuracy: 0.576\n",
            "Step: 1400 Loss: 4.421 Accuracy: 0.576\n",
            "Step: 1500 Loss: 4.430 Accuracy: 0.577\n",
            " ... Held-out nats: -0.894\n",
            "saved /content/bayesian_amazon/step01599_weights.png\n",
            "saved /content/bayesian_amazon/step01599_pred.png\n",
            "Step: 1600 Loss: 4.300 Accuracy: 0.577\n",
            "Step: 1700 Loss: 4.155 Accuracy: 0.577\n",
            "Step: 1800 Loss: 4.179 Accuracy: 0.579\n",
            "Step: 1900 Loss: 4.152 Accuracy: 0.579\n",
            " ... Held-out nats: -0.888\n",
            "saved /content/bayesian_amazon/step01999_weights.png\n",
            "saved /content/bayesian_amazon/step01999_pred.png\n",
            "Step: 2000 Loss: 3.968 Accuracy: 0.580\n",
            "Step: 2100 Loss: 3.898 Accuracy: 0.581\n",
            "Step: 2200 Loss: 3.861 Accuracy: 0.582\n",
            "Step: 2300 Loss: 3.887 Accuracy: 0.582\n",
            " ... Held-out nats: -0.883\n",
            "saved /content/bayesian_amazon/step02399_weights.png\n",
            "saved /content/bayesian_amazon/step02399_pred.png\n",
            "Step: 2400 Loss: 3.744 Accuracy: 0.582\n",
            "Step: 2500 Loss: 3.867 Accuracy: 0.582\n",
            "Step: 2600 Loss: 3.704 Accuracy: 0.582\n",
            "Step: 2700 Loss: 3.654 Accuracy: 0.583\n",
            " ... Held-out nats: -0.881\n",
            "saved /content/bayesian_amazon/step02799_weights.png\n",
            "saved /content/bayesian_amazon/step02799_pred.png\n",
            "Step: 2800 Loss: 3.493 Accuracy: 0.583\n",
            "Step: 2900 Loss: 3.571 Accuracy: 0.583\n",
            "Step: 3000 Loss: 3.490 Accuracy: 0.584\n",
            "Step: 3100 Loss: 3.424 Accuracy: 0.584\n",
            " ... Held-out nats: -0.881\n",
            "saved /content/bayesian_amazon/step03199_weights.png\n",
            "saved /content/bayesian_amazon/step03199_pred.png\n",
            "Step: 3200 Loss: 3.383 Accuracy: 0.584\n",
            "Step: 3300 Loss: 3.367 Accuracy: 0.585\n",
            "Step: 3400 Loss: 3.362 Accuracy: 0.585\n",
            "Step: 3500 Loss: 3.212 Accuracy: 0.585\n",
            " ... Held-out nats: -0.880\n",
            "saved /content/bayesian_amazon/step03599_weights.png\n",
            "saved /content/bayesian_amazon/step03599_pred.png\n",
            "Step: 3600 Loss: 3.231 Accuracy: 0.585\n",
            "Step: 3700 Loss: 3.115 Accuracy: 0.585\n",
            "Step: 3800 Loss: 3.119 Accuracy: 0.585\n",
            "Step: 3900 Loss: 3.105 Accuracy: 0.585\n",
            " ... Held-out nats: -0.878\n",
            "saved /content/bayesian_amazon/step03999_weights.png\n",
            "saved /content/bayesian_amazon/step03999_pred.png\n",
            "Step: 4000 Loss: 3.057 Accuracy: 0.585\n",
            "Step: 4100 Loss: 2.959 Accuracy: 0.586\n",
            "Step: 4200 Loss: 2.935 Accuracy: 0.586\n",
            "Step: 4300 Loss: 2.967 Accuracy: 0.586\n",
            " ... Held-out nats: -0.880\n",
            "saved /content/bayesian_amazon/step04399_weights.png\n",
            "saved /content/bayesian_amazon/step04399_pred.png\n",
            "Step: 4400 Loss: 3.008 Accuracy: 0.587\n",
            "Step: 4500 Loss: 2.874 Accuracy: 0.587\n",
            "Step: 4600 Loss: 2.751 Accuracy: 0.587\n",
            "Step: 4700 Loss: 2.704 Accuracy: 0.587\n",
            " ... Held-out nats: -0.887\n",
            "saved /content/bayesian_amazon/step04799_weights.png\n",
            "saved /content/bayesian_amazon/step04799_pred.png\n",
            "Step: 4800 Loss: 2.689 Accuracy: 0.587\n",
            "Step: 4900 Loss: 2.837 Accuracy: 0.587\n",
            "Step: 5000 Loss: 2.778 Accuracy: 0.587\n",
            "Step: 5100 Loss: 2.744 Accuracy: 0.587\n",
            " ... Held-out nats: -0.883\n",
            "saved /content/bayesian_amazon/step05199_weights.png\n",
            "saved /content/bayesian_amazon/step05199_pred.png\n",
            "Step: 5200 Loss: 2.640 Accuracy: 0.587\n",
            "Step: 5300 Loss: 2.693 Accuracy: 0.587\n",
            "Step: 5400 Loss: 2.528 Accuracy: 0.588\n",
            "Step: 5500 Loss: 2.607 Accuracy: 0.588\n",
            " ... Held-out nats: -0.881\n",
            "saved /content/bayesian_amazon/step05599_weights.png\n",
            "saved /content/bayesian_amazon/step05599_pred.png\n",
            "Step: 5600 Loss: 2.490 Accuracy: 0.588\n",
            "Step: 5700 Loss: 2.528 Accuracy: 0.588\n",
            "Step: 5800 Loss: 2.505 Accuracy: 0.588\n",
            "Step: 5900 Loss: 2.474 Accuracy: 0.588\n",
            " ... Held-out nats: -0.884\n",
            "saved /content/bayesian_amazon/step05999_weights.png\n",
            "saved /content/bayesian_amazon/step05999_pred.png\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}